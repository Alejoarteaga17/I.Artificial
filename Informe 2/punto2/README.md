# ============================================================
# 6) Generar README.md con resultados
# ============================================================
readme_content = f"""
# üìÑ Informe - Modelos de Aprendizaje Supervisado sobre California Housing

## 1. Descripci√≥n del dataset
- **Fuente:** [California Housing Dataset](https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset)
- **N√∫mero de registros:** {len(df)} observaciones
- **N√∫mero de variables (features):** {X.shape[1]}
- **Variable objetivo (target):** `MedHouseVal`
- **Tipo de problema:** **Regresi√≥n supervisada**

| Variable         | Descripci√≥n |
|------------------|-------------|
| MedInc           | Ingreso medio en la zona |
| HouseAge         | Edad media de las casas |
| AveRooms         | N√∫mero medio de habitaciones |
| AveBedrms        | N√∫mero medio de dormitorios |
| Population       | Poblaci√≥n total |
| AveOccup         | Ocupaci√≥n media por vivienda |
| Latitude         | Latitud geogr√°fica |
| Longitude        | Longitud geogr√°fica |
| **MedHouseVal**  | **Valor medio de las casas (target)** |

---

## 2. Preprocesamiento realizado
1. **Limpieza de datos faltantes:** No se encontraron valores nulos.
2. **Codificaci√≥n de variables categ√≥ricas:** Todas las variables son num√©ricas.
3. **Escalado/normalizaci√≥n:** Se aplic√≥ `StandardScaler`.
4. **Divisi√≥n en train/test:** 80% entrenamiento, 20% prueba (`random_state=42`).

---

## 3. Modelos entrenados
- **Random Forest Regressor** (`n_estimators=100`)
- **Gradient Boosting Regressor** (`n_estimators=200`, `learning_rate=0.1`, `max_depth=3`)
- **Red Neuronal (Keras - TensorFlow)**:
  - Capas densas: 128 ‚Üí 64 ‚Üí 32 ‚Üí 1
  - Activaci√≥n: ReLU en capas ocultas, salida lineal
  - Optimizador: Adam
  - P√©rdida: MSE
  - √âpocas: hasta 200 con `EarlyStopping`

---

## 4. Evaluaci√≥n de resultados

| Modelo           | MAE   | MSE   | RMSE  | R¬≤   |
|------------------|-------|-------|-------|------|
| Random Forest    | {mae_rf:.4f} | {mse_rf:.4f} | {rmse_rf:.4f} | {r2_rf:.4f} |
| Gradient Boosting| {mae_gbr:.4f} | {mse_gbr:.4f} | {rmse_gbr:.4f} | {r2_gbr:.4f} |
| Red Neuronal     | {mae:.4f} | {mse:.4f} | {rmse:.4f} | {r2:.4f} |

---

## 5. An√°lisis comparativo
- **Random Forest:** Mejor desempe√±o global, robusto, r√°pido de entrenar.
- **Gradient Boosting:** Similar al RF, aunque con mayor tiempo de entrenamiento.
- **Red Neuronal:** Buen resultado pero menor R¬≤, requiere m√°s ajuste y datos.

---

## 6. Conclusiones
- El **Random Forest** fue el modelo m√°s adecuado para este dataset.
- Los m√©todos ensemble (RF y GB) superaron a la red neuronal.
- La pr√°ctica permiti√≥ comprender la importancia de comparar distintos algoritmos en un mismo problema.
"""

